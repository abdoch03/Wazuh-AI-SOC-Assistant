import os
import requests
import json
import math
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta
from pymongo import MongoClient
import re

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from sentence_transformers import CrossEncoder

# --- Configuration am√©lior√©e ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise RuntimeError("‚ùå Erreur : La variable d'environnement GEMINI_API_KEY n'est pas d√©finie.")

MONGO_URI = os.getenv("MONGO_URI", "mongodb://localhost:27017/")
MONGO_DB_NAME = os.getenv("MONGO_DB_NAME", "wazuh_siem")
client = MongoClient(MONGO_URI)
db = client[MONGO_DB_NAME]

GEMINI_MODEL = "gemini-1.5-flash"
GEMINI_URL = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent"


CONFIG = {
    "max_recent_alerts": 1000,
    "max_query_results": 100,
    "rag_top_k": 10,
    "default_days_back": 7,
    "max_total_count_limit": 50000,
    "gemini_max_mongodb_results": 5,
    "gemini_max_rag_docs": 3,
    "gemini_max_chars_per_result": 300,
    "gemini_summary_max_chars": 800,
}

# --- Cache pour l'historique des conversations ---
conversation_history = {}

# --- Base vectorielle ---
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
VECTOR_DB_DIR = "./data_collecte/chroma_db"
vectorstore = Chroma(
    collection_name="wazuh_data",
    embedding_function=embeddings,
    persist_directory=VECTOR_DB_DIR
)

# --- Re-ranker optionnel ---
RERANKER_AVAILABLE = False
reranker = None

# =============================================================================
# GESTIONNAIRE INTELLIGENT DES DONN√âES
# =============================================================================

class DataManager:
    """Gestionnaire intelligent des donn√©es avec pagination et compression"""
    
    def __init__(self, config: dict):
        self.config = config
        self.limits = {
            "critical": {"max_items": 20, "chars_per_item": 600},
            "detailed": {"max_items": 15, "chars_per_item": 500},
            "standard": {"max_items": 10, "chars_per_item": 400},
            "summary": {"max_items": 8, "chars_per_item": 300}
        }
        
    def estimate_token_usage(self, text: str) -> int:
        """Estime l'usage de tokens (approximation : 1 token = 4 caract√®res)"""
        return len(text) // 4
    
    def determine_detail_level(self, question: str, data_count: int) -> str:
        """D√©termine le niveau de d√©tail requis selon la question"""
        question_lower = question.lower()
        
        # Questions critiques n√©cessitant le maximum de d√©tails
        critical_keywords = ["critique", "critical", "incident", "intrusion", "malware", "breach"]
        if any(kw in question_lower for kw in critical_keywords):
            return "critical"
        
        # Questions demandant explicitement des d√©tails
        detail_keywords = ["d√©tail", "detail", "complet", "full", "analyse", "investigation"]
        if any(kw in question_lower for kw in detail_keywords) or data_count <= 5:
            return "detailed"
        
        # Questions de r√©sum√©
        summary_keywords = ["r√©sum√©", "summary", "overview", "global", "g√©n√©ral"]
        if any(kw in question_lower for kw in summary_keywords) or data_count > 50:
            return "summary"
            
        return "standard"
    
    def smart_truncate(self, text: str, max_chars: int) -> str:
        """Troncature intelligente pr√©servant les informations importantes"""
        if len(text) <= max_chars:
            return text
            
        # Pr√©server les √©l√©ments importants (IPs, r√®gles, timestamps)
        important_patterns = [
            r'\d+\.\d+\.\d+\.\d+',  # IPs
            r'rule[_\s]*\d+',       # Rule IDs
            r'level[_\s]*\d+',      # Levels
            r'\d{4}-\d{2}-\d{2}',   # Dates
            r'agent[_\s]*\d+',      # Agent IDs
        ]
        
        # Tronquer en gardant le d√©but et la fin si possible
        if max_chars > 100:
            start_chars = int(max_chars * 0.7)
            end_chars = max_chars - start_chars - 3
            
            truncated = text[:start_chars] + "..." + text[-end_chars:]
            return truncated
        else:
            return text[:max_chars-3] + "..."
    
    def compress_data_item(self, item: Dict, max_chars: int, priority_fields: List[str] = None) -> str:
        """Compresse un √©l√©ment de donn√©es en gardant les champs prioritaires"""
        if not priority_fields:
            priority_fields = ['timestamp', 'agent_id', 'level', 'rule_description', 'rule_id']
        
        # Construire la repr√©sentation avec priorit√©s
        parts = []
        remaining_chars = max_chars
        
        for field in priority_fields:
            if field in item and remaining_chars > 0:
                value = str(item[field])
                if len(value) + len(field) + 3 <= remaining_chars:
                    parts.append(f"{field}: {value}")
                    remaining_chars -= len(parts[-1]) + 2
                else:
                    available = remaining_chars - len(field) - 5
                    if available > 10:
                        truncated_value = self.smart_truncate(value, available)
                        parts.append(f"{field}: {truncated_value}")
                    break
        
        # Ajouter d'autres champs si il reste de la place
        other_fields = [k for k in item.keys() if k not in priority_fields and k != '_id']
        for field in other_fields:
            if remaining_chars > len(field) + 10:
                value = str(item[field])
                available = min(remaining_chars - len(field) - 3, 50)
                if len(value) <= available:
                    parts.append(f"{field}: {value}")
                    remaining_chars -= len(parts[-1]) + 2
                else:
                    truncated_value = self.smart_truncate(value, available)
                    parts.append(f"{field}: {truncated_value}")
                    break
        
        return " | ".join(parts)
    
    def prepare_data(self, mongodb_data: List[Dict], query_type: str, question: str) -> str:
        """Pr√©pare les donn√©es de mani√®re intelligente selon le contexte"""
        if not mongodb_data:
            return "=== AUCUNE DONN√âE TROUV√âE ==="
        
        # D√©terminer le niveau de d√©tail requis
        detail_level = self.determine_detail_level(question, len(mongodb_data))
        limits = self.limits[detail_level]
        
        # Limiter le nombre d'√©l√©ments
        max_items = min(limits["max_items"], len(mongodb_data))
        selected_data = mongodb_data[:max_items]
        
        # Estimer l'espace disponible
        total_estimated_chars = limits["max_items"] * limits["chars_per_item"]
        
        # Pr√©parer les donn√©es
        formatted_items = []
        current_total_chars = 0
        
        # D√©finir les champs prioritaires selon le type de requ√™te
        priority_fields_map = {
            "ssh_failed": ['timestamp', 'srcip', 'agent_id', 'rule_description'],
            "critical_alerts": ['timestamp', 'level', 'agent_id', 'rule_description', 'rule_id'],
            "vulnerabilities": ['timestamp', 'agent_id', 'vulnerability', 'severity'],
            "fim_changes": ['timestamp', 'agent_id', 'syscheck', 'rule_description'],
            "network_activity": ['timestamp', 'srcip', 'dstip', 'agent_id', 'rule_description'],
            "default": ['timestamp', 'agent_id', 'level', 'rule_description', 'rule_id']
        }
        
        priority_fields = priority_fields_map.get(query_type, priority_fields_map["default"])
        
        for idx, item in enumerate(selected_data, 1):
            # Calculer l'espace disponible pour cet √©l√©ment
            remaining_items = len(selected_data) - idx + 1
            avg_chars_per_remaining = (total_estimated_chars - current_total_chars) // remaining_items
            max_chars_this_item = min(limits["chars_per_item"], avg_chars_per_remaining)
            
            # Comprimer l'√©l√©ment
            compressed_item = self.compress_data_item(item, max_chars_this_item, priority_fields)
            item_with_number = f"[{idx}] {compressed_item}"
            
            formatted_items.append(item_with_number)
            current_total_chars += len(item_with_number)
            
            # V√©rifier si on approche de la limite
            estimated_tokens = self.estimate_token_usage("\n".join(formatted_items))
            if estimated_tokens > 1500:
                break
        
        # Assembler le r√©sultat final
        header = f"=== {query_type.upper().replace('_', ' ')} - NIVEAU {detail_level.upper()} ==="
        summary = f"Affichage de {len(formatted_items)}/{len(mongodb_data)} √©l√©ments"
        
        if len(mongodb_data) > len(formatted_items):
            summary += f" (+ {len(mongodb_data) - len(formatted_items)} √©l√©ments suppl√©mentaires disponibles)"
        
        result = f"{header}\n{summary}\n\n" + "\n\n".join(formatted_items)
        
        # Ajouter statistiques si pertinentes
        if len(mongodb_data) > 5:
            stats = self.generate_quick_stats(mongodb_data)
            result += f"\n\n=== STATISTIQUES RAPIDES ===\n{stats}"
        
        return result
    
    def generate_quick_stats(self, data: List[Dict]) -> str:
        """G√©n√®re des statistiques rapides sur le dataset"""
        stats = []
        
        # Compter par level si disponible
        if data and 'level' in data[0]:
            levels = {}
            for item in data:
                level = item.get('level', 'Unknown')
                levels[level] = levels.get(level, 0) + 1
            stats.append(f"Niveaux: {dict(sorted(levels.items(), reverse=True))}")
        
        # Compter par agent si disponible
        if data and 'agent_id' in data[0]:
            agents = {}
            for item in data:
                agent = item.get('agent_id', 'Unknown')
                agents[agent] = agents.get(agent, 0) + 1
            top_agents = sorted(agents.items(), key=lambda x: x[1], reverse=True)[:3]
            stats.append(f"Top Agents: {dict(top_agents)}")
        
        # P√©riode couverte si timestamps disponibles
        if data and 'timestamp' in data[0]:
            timestamps = [item.get('timestamp') for item in data if item.get('timestamp')]
            if timestamps:
                stats.append(f"P√©riode: {min(timestamps)} √† {max(timestamps)}")
        
        return " | ".join(stats)

# Initialisation du gestionnaire
data_manager = DataManager(CONFIG)

class EnhancedDataFormatter:
    """Formateur de donn√©es sp√©cialis√© pour r√©ponses structur√©es"""
    
    def __init__(self):
        self.formatters = {
            "high_alerts": self.format_high_alerts,
            "critical_alerts": self.format_critical_alerts,
            "ssh_failed": self.format_ssh_failures,
            "fim_changes": self.format_fim_changes,
            "vulnerabilities": self.format_vulnerabilities,
            "network_activity": self.format_network_activity,
            "authentication": self.format_authentication,
            "malware_detection": self.format_malware_detection,
            "general_search": self.format_general_data,
            "agent_status": self.format_agent_status,
            "disconnected_agents": self.format_disconnected_agents
        }
    
    def format_data(self, data: List[Dict], query_type: str, question: str) -> str:
        """Point d'entr√©e principal pour le formatage"""
        if not data:
            return "=== AUCUNE DONN√âE TROUV√âE ==="
        
        formatter = self.formatters.get(query_type, self.format_general_data)
        return formatter(data, question)
    
    def format_high_alerts(self, data: List[Dict], question: str) -> str:
        """Format sp√©cialis√© pour alertes de niveau √©lev√©"""
        if not data:
            return "Aucune alerte de niveau √©lev√© trouv√©e"
        
        # En-t√™te avec statistiques
        total_count = len(data)
        level_stats = {}
        agent_stats = {}
        
        for alert in data:
            level = alert.get('level', 'Unknown')
            agent = alert.get('agent_id', 'Unknown')
            level_stats[level] = level_stats.get(level, 0) + 1
            agent_stats[agent] = agent_stats.get(agent, 0) + 1
        
        result = f"""=== DONN√âES RE√áUES - ALERTES NIVEAU √âLEV√â ===
Total trouv√©: {total_count} alertes
Niveaux: {dict(sorted(level_stats.items(), reverse=True))}
Agents concern√©s: {len(agent_stats)}
P√©riode: {data[-1].get('timestamp', 'N/A')} √† {data[0].get('timestamp', 'N/A')}

=== D√âTAILS DES ALERTES ==="""
        
        # Limiter √† 15 alertes pour lisibilit√© mais garder le maximum d'infos
        display_alerts = data[:15]
        
        for i, alert in enumerate(display_alerts, 1):
            timestamp = alert.get('timestamp', 'N/A')
            agent_id = alert.get('agent_id', 'N/A')
            agent_name = alert.get('agent_name', 'N/A')
            level = alert.get('level', 'N/A')
            rule_id = alert.get('rule_id', 'N/A')
            rule_desc = alert.get('rule_description', 'N/A')
            location = alert.get('location', 'N/A')
            
            # Extraire IP source si disponible
            srcip = alert.get('srcip', alert.get('data', {}).get('srcip', 'N/A'))
            
            # Log partiel pour contexte
            full_log = alert.get('full_log', '')
            log_preview = full_log[:150] + "..." if len(full_log) > 150 else full_log
            
            result += f"""

[{i}] ALERTE NIVEAU {level} - {timestamp}
‚îú‚îÄ‚îÄ Agent: {agent_id} ({agent_name})
‚îú‚îÄ‚îÄ R√®gle: {rule_id} - {rule_desc}
‚îú‚îÄ‚îÄ Localisation: {location}
‚îú‚îÄ‚îÄ IP Source: {srcip}
‚îî‚îÄ‚îÄ Log: {log_preview}"""
        
        if total_count > 15:
            result += f"\n\n... et {total_count - 15} autres alertes similaires"
        
        return result
    
    def format_critical_alerts(self, data: List[Dict], question: str) -> str:
        """Format sp√©cialis√© pour alertes critiques (niveau ‚â•13)"""
        
        return self.format_high_alerts(data, question).replace("NIVEAU √âLEV√â", "CRITIQUES (‚â•13)")
    
    def format_ssh_failures(self, data: List[Dict], question: str) -> str:
        """Format sp√©cialis√© pour √©checs SSH"""
        if not data:
            return "Aucun √©chec SSH d√©tect√©"
        
        # Analyse des IPs et patterns
        ip_attempts = {}
        time_pattern = {}
        
        for attempt in data:
            srcip = attempt.get('srcip', 'IP_Inconnue')
            timestamp = attempt.get('timestamp', '')
            
            ip_attempts[srcip] = ip_attempts.get(srcip, 0) + 1
            
            # Extraire heure pour pattern temporel
            if timestamp:
                hour = timestamp.split('T')[1][:2] if 'T' in timestamp else 'Unknown'
                time_pattern[hour] = time_pattern.get(hour, 0) + 1
        
        # Top IPs suspectes
        top_ips = sorted(ip_attempts.items(), key=lambda x: x[1], reverse=True)[:5]
        
        result = f"""=== DONN√âES RE√áUES - √âCHECS AUTHENTIFICATION SSH ===
Total tentatives: {len(data)}
IPs uniques: {len(ip_attempts)}
Top IPs suspectes: {dict(top_ips)}
P√©riode: {data[-1].get('timestamp', 'N/A')} √† {data[0].get('timestamp', 'N/A')}

=== D√âTAILS DES TENTATIVES ==="""
        
        # Afficher les 10 premiers √©checs avec contexte
        for i, attempt in enumerate(data[:10], 1):
            timestamp = attempt.get('timestamp', 'N/A')
            agent_id = attempt.get('agent_id', 'N/A')
            agent_name = attempt.get('agent_name', 'N/A')
            srcip = attempt.get('srcip', 'IP_Inconnue')
            rule_desc = attempt.get('rule_description', 'N/A')
            location = attempt.get('location', 'N/A')
            
            # Extraire utilisateur depuis le log si disponible
            full_log = attempt.get('full_log', '')
            user_match = re.search(r'user (\w+)', full_log) or re.search(r'for (\w+)', full_log)
            user = user_match.group(1) if user_match else 'Unknown'
            
            result += f"""

[{i}] √âCHEC SSH - {timestamp}
‚îú‚îÄ‚îÄ Agent: {agent_id} ({agent_name})
‚îú‚îÄ‚îÄ IP Source: {srcip} (Total: {ip_attempts.get(srcip, 1)} tentatives)
‚îú‚îÄ‚îÄ Utilisateur: {user}
‚îú‚îÄ‚îÄ Localisation: {location}
‚îî‚îÄ‚îÄ R√®gle: {rule_desc}"""
        
        if len(data) > 10:
            result += f"\n\n... et {len(data) - 10} autres tentatives"
        
        return result
    
    # Placeholder pour autres formatters
    def format_fim_changes(self, data: List[Dict], question: str) -> str:
        if not data:
            return "Aucune modification de fichier d√©tect√©e"
        
        # Analyser les patterns de modifications
        file_changes = {}
        directories = set()
        change_types = {}
        
        for change in data:
            # Extraire chemin fichier
            syscheck = change.get('syscheck', {})
            file_path = syscheck.get('path', change.get('location', 'Fichier_inconnu'))
            
            # Analyser le type de changement
            if 'added' in str(change).lower():
                change_type = 'Ajout√©'
            elif 'deleted' in str(change).lower() or 'removed' in str(change).lower():
                change_type = 'Supprim√©'
            elif 'modified' in str(change).lower():
                change_type = 'Modifi√©'
            else:
                change_type = 'Chang√©'
            
            file_changes[file_path] = file_changes.get(file_path, 0) + 1
            change_types[change_type] = change_types.get(change_type, 0) + 1
            
            # Extraire r√©pertoire
            if '/' in file_path:
                directory = '/'.join(file_path.split('/')[:-1])
                directories.add(directory)
        
        result = f"""=== DONN√âES RE√áUES - MODIFICATIONS FICHIERS (FIM) ===
    Total modifications: {len(data)}
    Fichiers uniques: {len(file_changes)}
    R√©pertoires concern√©s: {len(directories)}
    Types de changements: {dict(change_types)}
    P√©riode: {data[-1].get('timestamp', 'N/A')} √† {data[0].get('timestamp', 'N/A')}

    === D√âTAILS DES MODIFICATIONS ==="""
        
        for i, change in enumerate(data[:12], 1):
            timestamp = change.get('timestamp', 'N/A')
            agent_id = change.get('agent_id', 'N/A')
            agent_name = change.get('agent_name', 'N/A')
            
            # D√©tails syscheck
            syscheck = change.get('syscheck', {})
            file_path = syscheck.get('path', 'Fichier_inconnu')
            size_before = syscheck.get('size_before', 'N/A')
            size_after = syscheck.get('size_after', 'N/A')
            md5_before = syscheck.get('md5_before', 'N/A')[:8] if syscheck.get('md5_before') else 'N/A'
            md5_after = syscheck.get('md5_after', 'N/A')[:8] if syscheck.get('md5_after') else 'N/A'
            
            rule_desc = change.get('rule_description', 'Modification fichier')
            
            result += f"""

    [{i}] MODIFICATION FIM - {timestamp}
    ‚îú‚îÄ‚îÄ Agent: {agent_id} ({agent_name})
    ‚îú‚îÄ‚îÄ Fichier: {file_path}
    ‚îú‚îÄ‚îÄ Taille: {size_before} ‚Üí {size_after}
    ‚îú‚îÄ‚îÄ MD5: {md5_before} ‚Üí {md5_after}
    ‚îî‚îÄ‚îÄ R√®gle: {rule_desc}"""
        
        if len(data) > 12:
            result += f"\n\n... et {len(data) - 12} autres modifications"
        
        return result
    
    def format_vulnerabilities(self, data: List[Dict], question: str) -> str:
        if not data:
            return "Aucune vuln√©rabilit√© d√©tect√©e"
        
        # Analyser les vuln√©rabilit√©s
        cve_list = set()
        severity_stats = {}
        package_vulns = {}
        
        for vuln in data:
            vulnerability = vuln.get('vulnerability', {})
            
            # Extraire CVE
            cve = vulnerability.get('cve', vulnerability.get('id', 'CVE_inconnu'))
            if cve != 'CVE_inconnu':
                cve_list.add(cve)
            
            # S√©v√©rit√©
            severity = vulnerability.get('severity', vulnerability.get('level', 'Unknown'))
            severity_stats[severity] = severity_stats.get(severity, 0) + 1
            
            # Package affect√©
            package = vulnerability.get('package', {})
            pkg_name = package.get('name', 'Package_inconnu')
            if pkg_name != 'Package_inconnu':
                package_vulns[pkg_name] = package_vulns.get(pkg_name, 0) + 1
        
        result = f"""=== DONN√âES RE√áUES - VULN√âRABILIT√âS ===
    Total vuln√©rabilit√©s: {len(data)}
    CVE uniques: {len(cve_list)}
    S√©v√©rit√©s: {dict(sorted(severity_stats.items()))}
    Packages concern√©s: {len(package_vulns)}
    P√©riode: {data[-1].get('timestamp', 'N/A')} √† {data[0].get('timestamp', 'N/A')}

    === D√âTAILS DES VULN√âRABILIT√âS ==="""
        
        for i, vuln in enumerate(data[:10], 1):
            timestamp = vuln.get('timestamp', 'N/A')
            agent_id = vuln.get('agent_id', 'N/A')
            agent_name = vuln.get('agent_name', 'N/A')
            
            vulnerability = vuln.get('vulnerability', {})
            cve = vulnerability.get('cve', vulnerability.get('id', 'CVE_inconnu'))
            severity = vulnerability.get('severity', 'Unknown')
            score = vulnerability.get('cvss', {}).get('score', 'N/A')
            
            package = vulnerability.get('package', {})
            pkg_name = package.get('name', 'Package_inconnu')
            pkg_version = package.get('version', 'Version_inconnue')
            
            title = vulnerability.get('title', vuln.get('rule_description', 'Vuln√©rabilit√© d√©tect√©e'))
            
            result += f"""

    [{i}] VULN√âRABILIT√â - {timestamp}
    ‚îú‚îÄ‚îÄ Agent: {agent_id} ({agent_name})
    ‚îú‚îÄ‚îÄ CVE: {cve} (Score: {score})
    ‚îú‚îÄ‚îÄ S√©v√©rit√©: {severity}
    ‚îú‚îÄ‚îÄ Package: {pkg_name} v{pkg_version}
    ‚îî‚îÄ‚îÄ Description: {title[:80]}..."""
        
        if len(data) > 10:
            result += f"\n\n... et {len(data) - 10} autres vuln√©rabilit√©s"
        
        return result
    
    def format_network_activity(self, data: List[Dict], question: str) -> str:
        if not data:
            return "Aucune activit√© r√©seau d√©tect√©e"
        
        # Analyser patterns r√©seau
        src_ips = set()
        dst_ips = set()
        ports = set()
        protocols = set()
        
        for activity in data:
            if activity.get('srcip'):
                src_ips.add(activity['srcip'])
            if activity.get('dstip'):
                dst_ips.add(activity['dstip'])
            if activity.get('srcport'):
                ports.add(activity['srcport'])
            if activity.get('dstport'):
                ports.add(activity['dstport'])
            if activity.get('protocol'):
                protocols.add(activity['protocol'])
        
        result = f"""=== DONN√âES RE√áUES - ACTIVIT√â R√âSEAU ===
    Total √©v√©nements: {len(data)}
    IPs sources: {len(src_ips)}
    IPs destinations: {len(dst_ips)}
    Ports uniques: {len(ports)}
    Protocoles: {list(protocols) if protocols else ['N/A']}
    P√©riode: {data[-1].get('timestamp', 'N/A')} √† {data[0].get('timestamp', 'N/A')}

    === D√âTAILS ACTIVIT√â R√âSEAU ==="""
        
        for i, activity in enumerate(data[:10], 1):
            timestamp = activity.get('timestamp', 'N/A')
            agent_id = activity.get('agent_id', 'N/A')
            agent_name = activity.get('agent_name', 'N/A')
            
            srcip = activity.get('srcip', 'N/A')
            dstip = activity.get('dstip', 'N/A')
            srcport = activity.get('srcport', 'N/A')
            dstport = activity.get('dstport', 'N/A')
            protocol = activity.get('protocol', 'N/A')
            
            rule_desc = activity.get('rule_description', 'Activit√© r√©seau')
            level = activity.get('level', 'N/A')
            
            result += f"""

    [{i}] ACTIVIT√â R√âSEAU L{level} - {timestamp}
    ‚îú‚îÄ‚îÄ Agent: {agent_id} ({agent_name})
    ‚îú‚îÄ‚îÄ Connexion: {srcip}:{srcport} ‚Üí {dstip}:{dstport}
    ‚îú‚îÄ‚îÄ Protocole: {protocol}
    ‚îî‚îÄ‚îÄ R√®gle: {rule_desc}"""
        
        if len(data) > 10:
            result += f"\n\n... et {len(data) - 10} autres √©v√©nements r√©seau"
        
        return result

    def format_authentication(self, data: List[Dict], question: str) -> str:
        """Format pour √©v√©nements d'authentification"""
        if not data:
            return "Aucun √©v√©nement d'authentification d√©tect√©"
        
        # Analyser patterns d'authentification
        success_count = 0
        failure_count = 0
        users = set()
        src_ips = set()
        
        for auth in data:
            rule_desc = auth.get('rule_description', '').lower()
            full_log = auth.get('full_log', '').lower()
            
            if 'success' in rule_desc or 'successful' in rule_desc or 'accepted' in full_log:
                success_count += 1
            elif 'fail' in rule_desc or 'invalid' in rule_desc or 'denied' in full_log:
                failure_count += 1
            
            # Extraire utilisateur du log
            full_log_content = auth.get('full_log', '')
            user_patterns = [r'user (\w+)', r'for (\w+)', r'login (\w+)', r'user=(\w+)']
            for pattern in user_patterns:
                match = re.search(pattern, full_log_content)
                if match:
                    users.add(match.group(1))
                    break
            
            if auth.get('srcip'):
                src_ips.add(auth['srcip'])
        
        result = f"""=== DONN√âES RE√áUES - AUTHENTIFICATION ===
    Total √©v√©nements: {len(data)}
    Succ√®s: {success_count}
    √âchecs: {failure_count}
    Utilisateurs uniques: {len(users)}
    IPs sources: {len(src_ips)}
    P√©riode: {data[-1].get('timestamp', 'N/A')} √† {data[0].get('timestamp', 'N/A')}

    === D√âTAILS AUTHENTIFICATION ==="""
        
        for i, auth in enumerate(data[:12], 1):
            timestamp = auth.get('timestamp', 'N/A')
            agent_id = auth.get('agent_id', 'N/A')
            agent_name = auth.get('agent_name', 'N/A')
            
            srcip = auth.get('srcip', 'N/A')
            rule_desc = auth.get('rule_description', '√âv√©nement authentification')
            level = auth.get('level', 'N/A')
            location = auth.get('location', 'N/A')
            
            # Extraire utilisateur
            full_log = auth.get('full_log', '')
            user_match = re.search(r'user (\w+)|for (\w+)', full_log)
            user = user_match.group(1) or user_match.group(2) if user_match else 'Unknown'
            
            # D√©terminer statut
            status = "SUCCESS" if 'success' in rule_desc.lower() else "FAILURE" if 'fail' in rule_desc.lower() else "UNKNOWN"
            
            result += f"""

    [{i}] AUTH {status} L{level} - {timestamp}
    ‚îú‚îÄ‚îÄ Agent: {agent_id} ({agent_name})
    ‚îú‚îÄ‚îÄ Utilisateur: {user}
    ‚îú‚îÄ‚îÄ IP Source: {srcip}
    ‚îú‚îÄ‚îÄ Localisation: {location}
    ‚îî‚îÄ‚îÄ R√®gle: {rule_desc}"""
        
        if len(data) > 12:
            result += f"\n\n... et {len(data) - 12} autres √©v√©nements"
        
        return result

    def format_malware_detection(self, data: List[Dict], question: str) -> str:
        """Format pour d√©tection malware/rootkit"""
        if not data:
            return "Aucune d√©tection de malware/rootkit"
        
        # Analyser d√©tections malware
        detection_types = {}
        agents_affected = set()
        file_paths = set()
        
        for detection in data:
            rule_desc = detection.get('rule_description', '')
            full_log = detection.get('full_log', '')
            
            # Classifier le type de d√©tection
            if 'rootkit' in rule_desc.lower() or 'rootkit' in full_log.lower():
                det_type = 'Rootkit'
            elif 'malware' in rule_desc.lower() or 'virus' in rule_desc.lower():
                det_type = 'Malware'
            elif 'trojan' in rule_desc.lower():
                det_type = 'Trojan'
            elif 'suspicious' in rule_desc.lower():
                det_type = 'Suspicious'
            else:
                det_type = 'Unknown'
            
            detection_types[det_type] = detection_types.get(det_type, 0) + 1
            agents_affected.add(detection.get('agent_id', 'Unknown'))
            
            # Extraire chemin fichier si disponible
            file_match = re.search(r'/[/\w\.-]+', full_log)
            if file_match:
                file_paths.add(file_match.group(0))
        
        result = f"""=== DONN√âES RE√áUES - D√âTECTION MALWARE/ROOTKIT ===
    Total d√©tections: {len(data)}
    Types: {dict(detection_types)}
    Agents affect√©s: {len(agents_affected)}
    Fichiers suspects: {len(file_paths)}
    P√©riode: {data[-1].get('timestamp', 'N/A')} √† {data[0].get('timestamp', 'N/A')}

    === D√âTAILS DES D√âTECTIONS ==="""
        
        for i, detection in enumerate(data[:10], 1):
            timestamp = detection.get('timestamp', 'N/A')
            agent_id = detection.get('agent_id', 'N/A')
            agent_name = detection.get('agent_name', 'N/A')
            
            rule_id = detection.get('rule_id', 'N/A')
            rule_desc = detection.get('rule_description', 'D√©tection malware')
            level = detection.get('level', 'N/A')
            location = detection.get('location', 'N/A')
            
            # Extraire fichier suspect
            full_log = detection.get('full_log', '')
            file_match = re.search(r'/[/\w\.-]+', full_log)
            suspect_file = file_match.group(0) if file_match else 'N/A'
            
            log_preview = full_log[:100] + "..." if len(full_log) > 100 else full_log
            
            result += f"""

    [{i}] D√âTECTION L{level} - {timestamp}
    ‚îú‚îÄ‚îÄ Agent: {agent_id} ({agent_name})
    ‚îú‚îÄ‚îÄ R√®gle: {rule_id} - {rule_desc}
    ‚îú‚îÄ‚îÄ Localisation: {location}
    ‚îú‚îÄ‚îÄ Fichier suspect: {suspect_file}
    ‚îî‚îÄ‚îÄ Log: {log_preview}"""
        
        if len(data) > 10:
            result += f"\n\n... et {len(data) - 10} autres d√©tections"
        
        return result

    def format_agent_status(self, data: List[Dict], question: str) -> str:
        """Format pour statut des agents"""
        if not data:
            return "Aucun agent trouv√©"
        
        # Analyser statuts agents
        status_counts = {}
        os_types = {}
        versions = {}
        
        for agent in data:
            status = agent.get('status', 'Unknown')
            status_counts[status] = status_counts.get(status, 0) + 1
            
            # OS si disponible
            os_info = agent.get('os', {})
            if isinstance(os_info, dict):
                os_name = os_info.get('name', 'Unknown')
                os_types[os_name] = os_types.get(os_name, 0) + 1
            
            # Version agent
            version = agent.get('version', 'Unknown')
            versions[version] = versions.get(version, 0) + 1
        
        result = f"""=== DONN√âES RE√áUES - STATUT AGENTS ===
    Total agents: {len(data)}
    Statuts: {dict(status_counts)}
    Syst√®mes: {dict(list(os_types.items())[:5])}
    Versions: {dict(list(versions.items())[:3])}

    === D√âTAILS DES AGENTS ==="""
        
        for i, agent in enumerate(data[:15], 1):
            agent_id = agent.get('id', 'N/A')
            name = agent.get('name', 'N/A')
            ip = agent.get('ip', 'N/A')
            status = agent.get('status', 'Unknown')
            last_keep_alive = agent.get('lastKeepAlive', agent.get('last_keepalive', 'N/A'))
            version = agent.get('version', 'N/A')
            
            # Statut avec √©moji
            status_icon = "‚úÖ" if status == "active" else "‚ùå" if status == "disconnected" else "‚ö†Ô∏è"
            
            result += f"""

    [{i}] AGENT {agent_id} {status_icon}
    ‚îú‚îÄ‚îÄ Nom: {name}
    ‚îú‚îÄ‚îÄ IP: {ip}
    ‚îú‚îÄ‚îÄ Statut: {status.upper()}
    ‚îú‚îÄ‚îÄ Derni√®re activit√©: {last_keep_alive}
    ‚îî‚îÄ‚îÄ Version: {version}"""
        
        if len(data) > 15:
            result += f"\n\n... et {len(data) - 15} autres agents"
        
        return result

    def format_disconnected_agents(self, data: List[Dict], question: str) -> str:
        """Format pour agents d√©connect√©s"""
        if not data:
            return "Aucun agent d√©connect√©"
        
        # Analyser dur√©es de d√©connexion
        disconnection_times = {}
        os_affected = {}
        
        for agent in data:
            last_seen = agent.get('lastKeepAlive', agent.get('last_keepalive'))
            if last_seen:
                # Calculer dur√©e approximative (simplified)
                disconnection_times[agent.get('id', 'Unknown')] = last_seen
            
            # OS affect√©
            os_info = agent.get('os', {})
            if isinstance(os_info, dict):
                os_name = os_info.get('name', 'Unknown')
                os_affected[os_name] = os_affected.get(os_name, 0) + 1
        
        result = f"""=== DONN√âES RE√áUES - AGENTS D√âCONNECT√âS ===
    Total d√©connect√©s: {len(data)}
    Syst√®mes affect√©s: {dict(os_affected)}
    Statut: ATTENTION REQUISE

    === D√âTAILS DES AGENTS D√âCONNECT√âS ==="""
        
        for i, agent in enumerate(data[:12], 1):
            agent_id = agent.get('id', 'N/A')
            name = agent.get('name', 'N/A')
            ip = agent.get('ip', 'N/A')
            status = agent.get('status', 'disconnected')
            last_keep_alive = agent.get('lastKeepAlive', agent.get('last_keepalive', 'N/A'))
            
            # OS info
            os_info = agent.get('os', {})
            os_name = os_info.get('name', 'Unknown') if isinstance(os_info, dict) else 'Unknown'
            
            result += f"""

    [{i}] AGENT D√âCONNECT√â ‚ùå {agent_id}
    ‚îú‚îÄ‚îÄ Nom: {name}
    ‚îú‚îÄ‚îÄ IP: {ip}
    ‚îú‚îÄ‚îÄ OS: {os_name}
    ‚îú‚îÄ‚îÄ Statut: {status.upper()}
    ‚îî‚îÄ‚îÄ Derni√®re activit√©: {last_keep_alive}"""
        
        if len(data) > 12:
            result += f"\n\n... et {len(data) - 12} autres agents d√©connect√©s"
        
        # Ajouter section recommandations
        result += f"""

    === ACTIONS RECOMMAND√âES ===
    1. V√©rifier connectivit√© r√©seau des agents d√©connect√©s
    2. Red√©marrer service wazuh-agent sur les endpoints
    3. V√©rifier configuration firewall/proxy
    4. Contr√¥ler logs syst√®me des agents affect√©s"""
        
        return result

    def format_general_data(self, data: List[Dict], question: str) -> str:
        """Format g√©n√©rique am√©lior√© pour donn√©es non sp√©cialis√©es"""
        if not data:
            return "=== AUCUNE DONN√âE TROUV√âE ==="
        
        # Analyser structure des donn√©es pour formater intelligemment
        sample = data[0] if data else {}
        common_fields = ['timestamp', 'agent_id', 'level', 'rule_description', 'rule_id']
        available_fields = [field for field in common_fields if field in sample]
        
        result = f"""=== DONN√âES RE√áUES - RECHERCHE G√âN√âRALE ===
    Total √©l√©ments: {len(data)}
    Champs disponibles: {available_fields}
    P√©riode: {data[-1].get('timestamp', 'N/A')} √† {data[0].get('timestamp', 'N/A')}

    === APER√áU DES DONN√âES ==="""
        
        for i, item in enumerate(data[:10], 1):
            timestamp = item.get('timestamp', 'N/A')
            agent_id = item.get('agent_id', 'N/A')
            level = item.get('level', 'N/A')
            description = item.get('rule_description', item.get('message', str(item)[:100]))
            
            result += f"""

    [{i}] √âV√âNEMENT L{level} - {timestamp}
    ‚îú‚îÄ‚îÄ Agent: {agent_id}
    ‚îî‚îÄ‚îÄ Description: {description[:80]}..."""
        
        if len(data) > 10:
            result += f"\n\n... et {len(data) - 10} autres √©l√©ments"
        
        return result

enhanced_formatter = EnhancedDataFormatter()

def load_reranker():
    """Charge le re-ranker √† la demande avec gestion d'erreur robuste"""
    global reranker, RERANKER_AVAILABLE
    try:
        print("üîÑ Chargement du re-ranker CrossEncoder...")
        reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
        RERANKER_AVAILABLE = True
        print("‚úÖ Re-ranker Cross-Encoder activ√©")
        return True
    except ImportError as e:
        print(f"‚ö†Ô∏è D√©pendances manquantes pour re-ranker: {e}")
        RERANKER_AVAILABLE = False
        return False
    except Exception as e:
        print(f"‚ö†Ô∏è Re-ranker non disponible: {e}")
        RERANKER_AVAILABLE = False
        return False


# =============================================================================
# FONCTIONNALIT√âS PRINCIPALES
# =============================================================================

def call_gemini(prompt: str, conversation_id: str = None, 
                             temperature: float = 0.3, max_tokens: int = 2000) -> str:
    max_retries = 3
    base_delay = 2
    
    for attempt in range(max_retries + 1):
        try:
            # Calculer taille du prompt
            prompt_size = len(prompt)
            
            # Gestion intelligente des erreurs 429
            if attempt > 0:
                # R√©duire la taille du prompt si n√©cessaire
                if prompt_size > 4000:
                    prompt = compress_prompt_for_retry(prompt, target_size=3000)
                    print(f"üîß Prompt compress√©: {len(prompt)} caract√®res")
                
                # D√©lai exponentiel avec jitter
                delay = base_delay * (2 ** attempt) + (attempt * 0.5)
                print(f"‚è≥ Attente {delay:.1f}s avant retry {attempt}/{max_retries}...")
                import time
                time.sleep(delay)

            # Contexte conversationnel
            context_prompt = ""
            if conversation_id and conversation_id in conversation_history:
                recent_messages = conversation_history[conversation_id][-1:]
                if recent_messages:
                    last_msg = recent_messages[0]
                    context_prompt = f"CONTEXTE PR√âC√âDENT: {last_msg['question'][:100]}...\n\n"

            full_prompt = context_prompt + prompt

            headers = {"Content-Type": "application/json"}
            params = {"key": GEMINI_API_KEY}
            body = {
                "contents": [{"parts": [{"text": full_prompt}]}],
                "generationConfig": {
                    "temperature": temperature,
                    "maxOutputTokens": max_tokens,
                    "topP": 0.8,
                    "topK": 40
                },
                "safetySettings": [
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"}
                ]
            }

            response = requests.post(GEMINI_URL, headers=headers, params=params, 
                                   json=body, timeout=45)
            
            # Gestion sp√©cifique des codes d'erreur
            if response.status_code == 429:
                if attempt < max_retries:
                    print(f"‚ö†Ô∏è Rate limit atteint, retry {attempt + 1}/{max_retries}")
                    continue
                else:
                    return generate_fallback_response(prompt, "RATE_LIMIT")
            
            elif response.status_code == 503:
                if attempt < max_retries:
                    print(f"‚ö†Ô∏è Service indisponible, retry {attempt + 1}/{max_retries}")
                    continue
                else:
                    return generate_fallback_response(prompt, "SERVICE_UNAVAILABLE")
            
            response.raise_for_status()
            result = response.json()

            candidates = result.get("candidates", [])
            if not candidates:
                if attempt < max_retries:
                    continue
                return generate_fallback_response(prompt, "NO_CANDIDATES")

            parts = candidates[0].get("content", {}).get("parts", [])
            response_text = "".join([p.get("text", "") for p in parts]).strip()

            if response_text:
                return response_text
            elif attempt < max_retries:
                continue
            else:
                return generate_fallback_response(prompt, "EMPTY_RESPONSE")

        except requests.exceptions.Timeout:
            if attempt < max_retries:
                print(f"‚ö†Ô∏è Timeout, retry {attempt + 1}/{max_retries}")
                continue
            return generate_fallback_response(prompt, "TIMEOUT")
            
        except requests.exceptions.ConnectionError:
            if attempt < max_retries:
                print(f"‚ö†Ô∏è Erreur connexion, retry {attempt + 1}/{max_retries}")
                continue
            return generate_fallback_response(prompt, "CONNECTION_ERROR")
            
        except Exception as e:
            if attempt < max_retries:
                print(f"‚ö†Ô∏è Erreur inattendue: {e}, retry {attempt + 1}/{max_retries}")
                continue
            return generate_fallback_response(prompt, f"UNKNOWN_ERROR: {str(e)}")



def compress_prompt_for_retry(prompt: str, target_size: int = 3000) -> str:
    """Compresse intelligemment le prompt en cas d'erreur 429"""
    if len(prompt) <= target_size:
        return prompt
    
    lines = prompt.split('\n')
    essential_parts = []
    current_size = 0
    
    # Priorit√©s pour conserver les informations essentielles
    priority_keywords = ["QUESTION:", "DONN√âES D√âTAILL√âES:", "ALERTES:", "TIMESTAMP:", "AGENT:", "LEVEL:"]
    
    for line in lines:
        line_important = any(keyword in line.upper() for keyword in priority_keywords)
        
        if line_important or current_size < target_size * 0.7:
            if current_size + len(line) < target_size:
                essential_parts.append(line)
                current_size += len(line) + 1
            else:
                # Tronquer la ligne si n√©cessaire
                remaining = target_size - current_size - 10
                if remaining > 50:
                    essential_parts.append(line[:remaining] + "...")
                break
    
    return '\n'.join(essential_parts)

def generate_fallback_response(original_prompt: str, error_type: str) -> str:
    """G√©n√®re une r√©ponse de fallback bas√©e sur l'analyse basique du prompt"""
    fallback_responses = {
        "RATE_LIMIT": """
DONN√âES RE√áUES:
- Service temporairement surcharg√©
- Analyse basique effectu√©e sur votre requ√™te

ANALYSE RAPIDE:
Votre requ√™te concerne les alertes SOC. En raison de limitations temporaires du service d'analyse, 
voici les actions de base recommand√©es :
1. V√©rifier les logs Wazuh directement
2. Consulter le dashboard pour les alertes critiques
3. R√©essayer l'analyse dans quelques minutes

RECOMMANDATION: R√©it√©rez votre question dans 2-3 minutes pour une analyse compl√®te.
""",
        
        "SERVICE_UNAVAILABLE": "Service d'analyse IA temporairement indisponible. Consultez directement les logs Wazuh.",
        "TIMEOUT": "D√©lai d'analyse d√©pass√©. Requ√™te trop complexe, veuillez reformuler de mani√®re plus sp√©cifique.",
        "NO_CANDIDATES": "Aucune r√©ponse g√©n√©r√©e. V√©rifiez la formulation de votre question.",
        "EMPTY_RESPONSE": "R√©ponse vide re√ßue. Probl√®me technique temporaire."
    }
    
    return fallback_responses.get(error_type, f"Erreur technique: {error_type}")

def generate_optimized_prompt(question: str, formatted_data: str, rag_context: str, 
                            system_summary: str) -> str:
    """G√©n√®re un prompt optimis√© avec instructions claires pour le format de r√©ponse"""
    
    return f"""Tu es un SOC Analyst expert Wazuh. Ton r√¥le est d'analyser les donn√©es de s√©curit√© et fournir des r√©ponses structur√©es et actionnables.

QUESTION UTILISATEUR: "{question}"

{formatted_data}

CONTEXTE SYST√àME: {system_summary}
DOCUMENTATION: {rag_context}

INSTRUCTIONS CRITIQUES:
1. Structure ta r√©ponse EXACTEMENT comme suit:

SECTION 1 - SYNTH√àSE DES DONN√âES
‚Ä¢ R√©sume ce qui a √©t√© trouv√©
‚Ä¢ Indique les chiffres cl√©s
‚Ä¢ Mentionne la p√©riode couverte

SECTION 2 - ANALYSE APPROFONDIE  
‚Ä¢ Identifie les patterns suspects
‚Ä¢ Corr√®le les √©v√©nements
‚Ä¢ √âvalue le niveau de risque
‚Ä¢ Propose 3 actions prioritaires avec commandes pr√©cises

2. Soit pr√©cis et technique
3. Base ton analyse UNIQUEMENT sur les donn√©es fournies
4. Si donn√©es insuffisantes, demande des pr√©cisions sp√©cifiques
5. Inclus des commandes Wazuh pratiques

R√âPONSE:"""


def store_conversation(conversation_id: str, question: str, response: str):
    """Stocke l'√©change dans l'historique de conversation"""
    if conversation_id not in conversation_history:
        conversation_history[conversation_id] = []

    conversation_history[conversation_id].append({
        'question': question,
        'response': response,
        'timestamp': datetime.now().isoformat()
    })

    # Garder seulement les 10 derniers √©changes
    if len(conversation_history[conversation_id]) > 10:
        conversation_history[conversation_id] = conversation_history[conversation_id][-10:]

def extract_key_terms(text: str) -> List[str]:
    """Extrait les termes cl√©s d'un texte pour le contexte"""
    important_terms = {
        'ssh', 'alert', 'agent', 'critical', 'rule', 'network', 'authentication',
        'failed', 'success', 'malware', 'vulnerability', 'intrusion', 'security',
        'level', 'monitoring', 'detection', 'incident', 'firewall'
    }

    words = re.findall(r'\b\w+\b', text.lower())
    key_terms = [word for word in words if word in important_terms]

    return list(set(key_terms))

def retrieve_with_rag(question: str, top_k: int = None, conversation_id: str = None) -> List[Dict]:
    """RAG am√©lior√© avec re-ranker intelligent et gestion d'erreurs"""
    if top_k is None:
        top_k = CONFIG["rag_top_k"]

    try:
        # Enrichissement contextuel de la requ√™te
        enhanced_query = question
        
        # Ajouter contexte conversationnel
        if conversation_id and conversation_id in conversation_history:
            recent_context = conversation_history[conversation_id][-1:]
            if recent_context:
                context_keywords = extract_key_terms(recent_context[0]['question'])
                enhanced_query += f" {' '.join(context_keywords[:3])}"

        # Expansion s√©mantique sp√©cialis√©e SOC
        query_expansions = {
            "alerte": "alert detection monitoring incident security critical level high",
            "agent": "endpoint host system monitoring status connection",
            "niveau": "level priority severity critical high medium low",
            "ssh": "authentication connection login security access remote",
            "r√©seau": "network traffic connection firewall intrusion detection",
            "fichier": "file integrity monitoring syscheck modification change",
            "vuln√©rabilit√©": "vulnerability cve security patch exploit weakness"
        }
        
        question_lower = question.lower()
        for keyword, expansion in query_expansions.items():
            if keyword in question_lower:
                enhanced_query += f" {expansion}"

        print(f"üîé RAG recherche enrichie: {enhanced_query[:100]}...")

        # Recherche vectorielle initiale (plus large si re-ranker disponible)
        search_k = top_k * 4 if RERANKER_AVAILABLE else top_k
        docs = vectorstore.similarity_search(enhanced_query, k=search_k)

        if not docs:
            print("‚ö†Ô∏è Aucun document RAG trouv√©")
            return []

        # Re-ranking intelligent si disponible
        if RERANKER_AVAILABLE and reranker is not None and len(docs) > top_k:
            print(f"üéØ Re-ranking {len(docs)} documents...")
            
            try:
                # Pr√©parer les paires (query, document) pour le re-ranker
                pairs = []
                valid_docs = []
                
                for doc in docs:
                    if doc.page_content and len(doc.page_content.strip()) > 20:
                        pairs.append((enhanced_query, doc.page_content[:500]))
                        valid_docs.append(doc)
                
                if pairs:
                    scores = reranker.predict(pairs)
                    
                    # Combiner documents et scores
                    scored_docs = list(zip(valid_docs, scores))
                    scored_docs.sort(key=lambda x: x[1], reverse=True)
                    
                    # S√©lectionner les meilleurs
                    docs = [doc for doc, score in scored_docs[:top_k]]
                    print(f"‚úÖ Re-ranking termin√©, {len(docs)} docs s√©lectionn√©s")
                else:
                    docs = docs[:top_k]
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur re-ranking: {e}, utilisation ordre original")
                docs = docs[:top_k]
        else:
            docs = docs[:top_k]

        return [{"text": d.page_content, "metadata": d.metadata} for d in docs]

    except Exception as e:
        print(f"‚ùå Erreur recherche RAG: {e}")
        # Fallback avec documentation minimale
        return [{
            "text": "Documentation Wazuh de base disponible pour analyse SOC",
            "metadata": {"type": "fallback", "source": "internal"}
        }]


def truncate_text(text: str, max_chars: int) -> str:
    """Tronque intelligemment le texte"""
    if len(text) <= max_chars:
        return text

    truncated = text[:max_chars]
    last_period = truncated.rfind('.')
    last_space = truncated.rfind(' ')

    if last_period > max_chars * 0.8:
        return truncated[:last_period + 1] + "..."
    elif last_space > max_chars * 0.9:
        return truncated[:last_space] + "..."
    else:
        return truncated + "..."

def get_total_counts() -> Dict[str, int]:
    """R√©cup√®re les comptages totaux avec limitation de s√©curit√©"""
    try:
        counts = {}

        alerts_collection = db["alerts"]
        total_alerts = alerts_collection.count_documents({})

        if total_alerts > CONFIG["max_total_count_limit"]:
            total_alerts = alerts_collection.estimated_document_count()
            counts["alerts_estimated"] = True
        else:
            counts["alerts_estimated"] = False

        counts["total_alerts"] = total_alerts

        agents_collection = db["agents"]
        counts["total_agents"] = agents_collection.count_documents({})
        counts["active_agents"] = agents_collection.count_documents({"status": "active"})
        counts["disconnected_agents"] = agents_collection.count_documents({"status": {"$ne": "active"}})

        return counts

    except Exception as e:
        print(f"‚ùå Erreur comptage total: {e}")
        return {"error": str(e)}

def get_mongodb_summary_compact(days_back: int = None) -> str:
    """Version compacte du r√©sum√© pour Gemini"""
    try:
        if days_back is None:
            days_back = CONFIG["default_days_back"]

        summary = []

        total_counts = get_total_counts()
        if "error" not in total_counts:
            if total_counts.get("alerts_estimated"):
                summary.append(f"Total: ~{total_counts['total_alerts']} alertes (estim.)")
            else:
                summary.append(f"Total: {total_counts['total_alerts']} alertes")
            summary.append(f"Agents: {total_counts['active_agents']}/{total_counts['total_agents']} actifs")

        alerts_collection = db["alerts"]
        date_cutoff = datetime.now() - timedelta(days=days_back)

        # Utiliser timestamp au lieu de ingested_at pour la coh√©rence
        pipeline = [
            {"$match": {"timestamp": {"$gte": date_cutoff.isoformat()}}},
            {"$group": {
                "_id": "$level",
                "count": {"$sum": 1}
            }},
            {"$sort": {"_id": -1}},
            {"$limit": 5}
        ]

        level_results = list(alerts_collection.aggregate(pipeline))

        if level_results:
            summary.append(f"Alertes {days_back}j:")
            for result in level_results:
                level = result["_id"]
                count = result["count"]
                severity = "CRIT" if level >= 13 else "ELEV" if level >= 9 else "MOY" if level >= 5 else "INFO"
                summary.append(f"L{level}({severity}):{count}")

        full_summary = " | ".join(summary)
        return truncate_text(full_summary, CONFIG["gemini_summary_max_chars"])

    except Exception as e:
        return f"Erreur donn√©es: {e}"

def execute_predefined_query(question: str, custom_limit: int = None) -> Optional[Tuple[List[Dict], str]]:
    """"Version qui r√©cup√®re toujours tous les d√©tails disponibles"""
    question_lower = question.lower()
    limit = custom_limit or CONFIG["max_query_results"]

    try:
        collection = db["alerts"]
        
        # Projection compl√®te - r√©cup√©rer TOUS les champs importants
        full_projection = {
            "_id": 0, 
            "timestamp": 1, 
            "agent_id": 1, 
            "agent_name": 1,
            "level": 1,
            "rule_id": 1,
            "rule_description": 1, 
            "decoder_name": 1,
            "location": 1,
            "srcip": 1, 
            "dstip": 1,
            "srcport": 1,
            "dstport": 1,
            "protocol": 1,
            "full_log": 1,  # TOUJOURS inclure le log complet
            "syscheck": 1,
            "vulnerability": 1,
            "data": 1,
            "status": 1,
            "url": 1,
            "id": 1
        }

        # === SSH √âCHECS ===
        if any(keyword in question_lower for keyword in ["ssh", "connexion"]) and \
                any(keyword in question_lower for keyword in ["√©chec", "failed", "√©chou√©"]):
            print(f"üîç Recherche SSH √©checs (d√©tails complets)...")
            results = list(collection.find(
                {
                    "$or": [
                        {"rule_description": {"$regex": "Failed.*[Ss][Ss][Hh]|[Ss][Ss][Hh].*Failed", "$options": "i"}},
                        {"rule_id": {"$in": ["5710", "5711", "5712", "5713", "5714", "5716"]}}
                    ]
                },
                full_projection
            ).sort("timestamp", -1).limit(limit))
            return results, "ssh_failed"

        # Voici les parties manquantes √† ajouter dans execute_predefined_query apr√®s la section SSH :

        # === ALERTES CRITIQUES ===
        elif any(keyword in question_lower for keyword in ["critique", "critical", "urgent"]) and \
                any(keyword in question_lower for keyword in ["alerte", "alert", "√©v√©nement", "event"]):
            print(f"üîç Recherche alertes critiques (d√©tails complets)...")
            results = list(collection.find(
                {"level": {"$gte": 13}},
                full_projection
            ).sort("timestamp", -1).limit(limit))
            return results, "critical_alerts"

        # === ALERTES √âLEV√âES ===
        elif any(keyword in question_lower for keyword in ["√©lev√©", "high", "important"]) and \
                any(keyword in question_lower for keyword in ["alerte", "alert", "niveau", "level"]):
            print(f"üîç Recherche alertes √©lev√©es (d√©tails complets)...")
            results = list(collection.find(
                {"level": {"$gte": 9, "$lt": 13}},
                full_projection
            ).sort("timestamp", -1).limit(limit))
            return results, "high_alerts"

        # === MODIFICATIONS FICHIERS (FIM) ===
        elif any(keyword in question_lower for keyword in ["fichier", "file", "syscheck", "fim"]) and \
                any(keyword in question_lower for keyword in ["modif", "change", "int√©grit√©", "integrity"]):
            print(f"üîç Recherche modifications fichiers FIM (d√©tails complets)...")
            results = list(collection.find(
                {
                    "$or": [
                        {"decoder_name": "syscheck"},
                        {"rule_id": {"$in": ["550", "551", "552", "553", "554"]}},
                        {"syscheck": {"$exists": True}}
                    ]
                },
                full_projection
            ).sort("timestamp", -1).limit(limit))
            return results, "fim_changes"

        # === VULN√âRABILIT√âS ===
        elif any(keyword in question_lower for keyword in ["vuln√©rabilit√©", "vulnerability", "cve", "faille"]):
            print(f"üîç Recherche vuln√©rabilit√©s (d√©tails complets)...")
            results = list(collection.find(
                {
                    "$or": [
                        {"decoder_name": "vulnerability-detector"},
                        {"vulnerability": {"$exists": True}},
                        {"rule_description": {"$regex": "CVE|vulnerability", "$options": "i"}}
                    ]
                },
                full_projection
            ).sort("timestamp", -1).limit(limit))
            return results, "vulnerabilities"

        # === ACTIVIT√â R√âSEAU ===
        elif any(keyword in question_lower for keyword in ["r√©seau", "network", "connexion", "connection"]):
            print(f"üîç Recherche activit√© r√©seau (d√©tails complets)...")
            results = list(collection.find(
                {
                    "$or": [
                        {"srcip": {"$exists": True}},
                        {"dstip": {"$exists": True}},
                        {"rule_description": {"$regex": "network|connection|traffic", "$options": "i"}}
                    ]
                },
                full_projection
            ).sort("timestamp", -1).limit(limit))
            return results, "network_activity"

        # === AGENTS ===
        elif any(keyword in question_lower for keyword in ["agent", "endpoint"]):
            if "d√©connect√©" in question_lower or "disconnect" in question_lower:
                print(f"üîç Recherche agents d√©connect√©s...")
                agents_collection = db["agents"]
                results = list(agents_collection.find(
                    {"status": {"$ne": "active"}},
                    {"_id": 0, "id": 1, "name": 1, "ip": 1, "status": 1, "lastKeepAlive": 1}
                ).limit(limit))
                return results, "disconnected_agents"
            else:
                print(f"üîç Recherche statut agents...")
                agents_collection = db["agents"]
                results = list(agents_collection.find(
                    {},
                    {"_id": 0, "id": 1, "name": 1, "ip": 1, "status": 1, "lastKeepAlive": 1}
                ).limit(limit))
                return results, "agent_status"

        # === AUTHENTIFICATION ===
        elif any(keyword in question_lower for keyword in ["auth", "login", "connexion", "authentication"]):
            print(f"üîç Recherche √©v√©nements authentification (d√©tails complets)...")
            results = list(collection.find(
                {
                    "$or": [
                        {"decoder_name": {"$in": ["sshd", "pam", "windows"]}},
                        {"rule_description": {"$regex": "authentication|login|logon", "$options": "i"}}
                    ]
                },
                full_projection
            ).sort("timestamp", -1).limit(limit))
            return results, "authentication"

        # === MALWARE/ROOTCHECK ===
        elif any(keyword in question_lower for keyword in ["malware", "virus", "rootkit", "rootcheck"]):
            print(f"üîç Recherche d√©tection malware/rootkit (d√©tails complets)...")
            results = list(collection.find(
                {
                    "$or": [
                        {"decoder_name": "rootcheck"},
                        {"rule_id": {"$regex": "^51[0-9]"}},  # R√®gles rootcheck 510-519
                        {"rule_description": {"$regex": "malware|virus|rootkit|trojan", "$options": "i"}}
                    ]
                },
                full_projection
            ).sort("timestamp", -1).limit(limit))
            return results, "malware_detection"

        # === COMPTAGES SP√âCIAUX ===
        elif any(keyword in question_lower for keyword in ["combien", "nombre", "total", "count"]):
            if "agent" in question_lower:
                print(f"üîç Comptage agents...")
                agents_collection = db["agents"]
                total_agents = agents_collection.count_documents({})
                active_agents = agents_collection.count_documents({"status": "active"})
                disconnected_agents = total_agents - active_agents
                
                return [{
                    "total_agents": total_agents,
                    "active_agents": active_agents,
                    "disconnected_agents": disconnected_agents
                }], "agent_count"
            else:
                print(f"üîç Comptage total alertes...")
                total_alerts = collection.estimated_document_count()
                return [{"total_alerts": total_alerts, "estimated": True}], "total_count"

        # === RECHERCHE G√âN√âRALE ===
        else:
            print("üîç Recherche g√©n√©rale (d√©tails complets)...")
            # Recherche par mots-cl√©s dans rule_description et full_log
            search_terms = [term for term in question_lower.split() if len(term) > 2]
            
            if not search_terms:
                # Si pas de termes valides, retourner les derni√®res alertes
                results = list(collection.find(
                    {},
                    full_projection
                ).sort("timestamp", -1).limit(limit))
                return results, "general_search"
            
            # √âchapper les caract√®res sp√©ciaux pour les regex
            escaped_terms = []
            for term in search_terms:
                # √âchapper les caract√®res sp√©ciaux des regex
                escaped_term = re.escape(term)
                escaped_terms.append(escaped_term)
            
            # Cr√©er une regex valide
            try:
                regex_pattern = "|".join(escaped_terms)
                search_query = {
                    "$or": [
                        {"rule_description": {"$regex": regex_pattern, "$options": "i"}},
                        {"full_log": {"$regex": regex_pattern, "$options": "i"}}
                    ]
                }
                
                results = list(collection.find(
                    search_query,
                    full_projection
                ).sort("timestamp", -1).limit(limit))
                return results, "general_search"
                
            except Exception as regex_error:
                print(f"‚ö†Ô∏è Erreur regex, utilisation de la recherche simple: {regex_error}")
                # Fallback: recherche textuelle simple
                results = list(collection.find(
                    {},
                    full_projection
                ).sort("timestamp", -1).limit(limit))
                return results, "general_search"

        return None, None

    except Exception as e:
        print(f"‚ùå Erreur requ√™te MongoDB d√©taill√©e: {e}")
        return None, None

def add_wazuh_documentation():
    """Documentation SOC compl√®te pour combler les lacunes RAG"""
    print("üìö Ajout documentation SOC compl√®te...")

    comprehensive_docs = [
        # === FIM (File Integrity Monitoring) ===
        {
            "text": "Surveillance int√©grit√© fichiers Wazuh (FIM/syscheck): Module syscheckd surveille modifications /etc/passwd, /etc/shadow, /var/www/html (webshells). R√®gles 550-554 pour changements fichiers. Configuration: <syscheck><directories>/etc,/var/www</directories></syscheck>. Investigation: analyser checksums, timestamps, permissions modifi√©es. Webshells communs: shell.php, cmd.asp, backdoor.jsp.",
            "metadata": {"type": "doc", "topic": "fim_syscheck", "category": "security", "priority": "high"}
        },

        # === VULN√âRABILIT√âS ===
        {
            "text": "D√©tection vuln√©rabilit√©s Wazuh: Module vulnerability-detector scanne packages install√©s contre base CVE. Configuration wodles vulnerability-detector pour Ubuntu/CentOS/Windows. Alertes CVE critiques: CVSS >= 7.0 n√©cessitent patch prioritaire. Investigation: wazuh-ctl vulnerability-detector --list-cve, v√©rifier version package vuln√©rable, planifier mise √† jour syst√®me.",
            "metadata": {"type": "doc", "topic": "vulnerability_detection", "category": "security", "priority": "critical"}
        },

        # === ROOTCHECK/MALWARE ===
        {
            "text": "D√©tection rootkits/malware Wazuh: Module rootcheck cherche processus cach√©s, ports suspects, fichiers rootkit signatures. R√®gles 510-514 rootcheck. D√©tection: ps aux vs /proc differences, netstat ports cach√©s, checksums syst√®me modifi√©s. Investigation malware: isolated filesystem, memory dump analysis, C2 communication patterns.",
            "metadata": {"type": "doc", "topic": "rootcheck_malware", "category": "security", "priority": "high"}
        },

        # === R√âSEAU ===
        {
            "text": "Surveillance r√©seau Wazuh: Int√©gration Suricata/Zeek pour IDS. D√©tection scans ports (nmap, masscan), connexions C2 suspectes, exfiltration donn√©es. R√®gles 4100+ pour activit√©s r√©seau. Investigation: analyser flows, g√©olocalisation IPs, patterns temporels connexions, volumes transferts anormaux.",
            "metadata": {"type": "doc", "topic": "network_monitoring", "category": "security", "priority": "medium"}
        },

        # === CONFORMIT√â ===
        {
            "text": "Conformit√© Wazuh CIS/HIPAA/PCI-DSS: Module SCA (Security Configuration Assessment) v√©rifie benchmarks s√©curit√©. Politiques CIS pour Linux/Windows, contr√¥les HIPAA donn√©es m√©dicales, requirements PCI cartes bancaires. R√®gles 2900+ conformit√©. Investigation: gap analysis, remediation prioritaire, audit trails.",
            "metadata": {"type": "doc", "topic": "compliance_sca", "category": "compliance", "priority": "medium"}
        },

        # === CORR√âLATION AVANC√âE ===
        {
            "text": "Corr√©lation √©v√©nements SOC: Corr√©ler agent d√©connect√© + CVE critique + tentatives SSH = compromission probable. Timeline reconstruction: login suspect ‚Üí escalade privil√®ges ‚Üí persistence ‚Üí exfiltration. Investigation playbook: isoler agent, snapshot forensique, analyser artifacts, contenir menace, √©radiquer, r√©cup√©rer.",
            "metadata": {"type": "doc", "topic": "correlation_investigation", "category": "incident_response", "priority": "critical"}
        }
    ]

    try:
        texts = [doc["text"] for doc in comprehensive_docs]
        metadatas = [doc["metadata"] for doc in comprehensive_docs]
        vectorstore.add_texts(texts, metadatas=metadatas)
        print(f"‚úÖ {len(texts)} documents SOC complets ajout√©s")
        return True
    except Exception as e:
        print(f"‚ùå Erreur ajout documentation SOC: {e}")
        return False

def generate_soc_dashboard_summary() -> str:
    """G√©n√®re un r√©sum√© dashboard SOC multi-sources"""
    try:
        summary = []

        # === 1. ALERTES CRITIQUES ===
        collection = db["alerts"]
        critical_count = collection.count_documents({"level": {"$gte": 13}})
        summary.append(f"üö® Critiques: {critical_count}")

        # === 2. AGENTS D√âCONNECT√âS ===
        agents_collection = db["agents"]
        disconnected = agents_collection.count_documents({"status": {"$ne": "active"}})
        total_agents = agents_collection.count_documents({})
        summary.append(f"üì° Agents: {total_agents - disconnected}/{total_agents}")

        # === 3. MODIFICATIONS FICHIERS (FIM) ===
        fim_count = collection.count_documents({
            "$or": [
                {"decoder_name": "syscheck"},
                {"rule_id": {"$in": ["550", "551", "552", "553", "554"]}}
            ]
        })
        summary.append(f"üìÅ FIM: {fim_count}")

        # === 4. VULN√âRABILIT√âS ===
        vuln_count = collection.count_documents({
            "$or": [
                {"decoder_name": "vulnerability-detector"},
                {"vulnerability": {"$exists": True}}
            ]
        })
        summary.append(f"üîì CVE: {vuln_count}")

        # === 5. SSH SUSPECTS ===
        ssh_failed = collection.count_documents({
            "rule_id": {"$in": ["5710", "5711", "5712", "5713", "5714", "5716"]}
        })
        summary.append(f"üîë SSH: {ssh_failed}")

        return " | ".join(summary)

    except Exception as e:
        return f"‚ùå Dashboard indisponible: {e}"

def format_soc_analysis_data(limited_data: List[Dict], total_count: int) -> str:
    """Formate les donn√©es pour analyse SOC g√©n√©rale"""
    if not limited_data:
        return "Aucun √©v√©nement SOC d√©tect√©"

    formatted_items = []
    for item in limited_data:
        # Extraire les champs les plus pertinents pour SOC
        timestamp = item.get('timestamp', 'N/A')
        agent_id = item.get('agent_id', 'N/A')
        level = item.get('level', 'N/A')
        description = item.get('rule_description', 'N/A')

        # R√©sum√© compact
        summary = f"[{timestamp}] Agent:{agent_id} L{level} - {description[:100]}"
        formatted_items.append(summary)

    result = f"ANALYSE SOC ({len(limited_data)}/{total_count} √©v√©nements):\n"
    result += "\n".join(formatted_items)

    if total_count > len(limited_data):
        result += f"\n... et {total_count - len(limited_data)} autres √©v√©nements"

    return result

def format_ssh_failures_data(limited_data: List[Dict], total_count: int) -> str:
    """Formate sp√©cifiquement les √©checs SSH"""
    if not limited_data:
        return "Aucun √©chec SSH d√©tect√©"

    formatted_items = []
    ip_counts = {}  # Compter tentatives par IP

    for item in limited_data:
        timestamp = item.get('timestamp', 'N/A')
        agent_id = item.get('agent_id', 'N/A')
        # G√©rer les diff√©rentes cl√©s possibles pour l'IP source
        srcip = item.get('srcip', item.get('src_ip', 'IP_inconnue'))

        # Compter les IPs
        ip_counts[srcip] = ip_counts.get(srcip, 0) + 1

        summary = f"[{timestamp}] Agent:{agent_id} depuis {srcip}"
        formatted_items.append(summary)

    result = f"√âCHECS SSH ({len(limited_data)}/{total_count} tentatives):\n"
    result += "\n".join(formatted_items[:5])  # Limiter √† 5 pour lisibilit√©

    # Ajouter top IPs suspectes
    if ip_counts:
        top_ips = sorted(ip_counts.items(), key=lambda x: x[1], reverse=True)[:3]
        result += f"\nTOP IPs suspectes: {', '.join([f'{ip}({count})' for ip, count in top_ips])}"

    return result

def format_critical_alerts_data(limited_data: List[Dict], total_count: int) -> str:
    """Formate sp√©cifiquement les alertes critiques"""
    if not limited_data:
        return "Aucune alerte critique d√©tect√©e"

    formatted_items = []
    level_counts = {}

    for item in limited_data:
        timestamp = item.get('timestamp', 'N/A')
        agent_id = item.get('agent_id', 'N/A')
        level = item.get('level', 'N/A')
        description = item.get('rule_description', 'N/A')

        level_counts[level] = level_counts.get(level, 0) + 1

        summary = f"[{timestamp}] Agent:{agent_id} NIVEAU-{level} - {description[:80]}"
        formatted_items.append(summary)

    result = f"ALERTES CRITIQUES ({len(limited_data)}/{total_count}):\n"
    result += "\n".join(formatted_items)

    # Ajouter r√©partition par niveau
    if level_counts:
        levels_summary = ", ".join([f"L{level}:{count}" for level, count in sorted(level_counts.items(), reverse=True)])
        result += f"\nR√©partition: {levels_summary}"

    return result

def format_fim_data(limited_data: List[Dict], total_count: int) -> str:
    """Formate les donn√©es FIM (File Integrity Monitoring)"""
    if not limited_data:
        return "Aucune modification de fichier d√©tect√©e"

    formatted_items = []
    file_paths = set()

    for item in limited_data:
        timestamp = item.get('timestamp', 'N/A')
        agent_id = item.get('agent_id', 'N/A')

        # Extraire chemin fichier si disponible
        syscheck_data = item.get('syscheck', {})
        file_path = syscheck_data.get('path', 'fichier_inconnu')
        file_paths.add(file_path)

        summary = f"[{timestamp}] Agent:{agent_id} - {file_path}"
        formatted_items.append(summary)

    result = f"MODIFICATIONS FICHIERS FIM ({len(limited_data)}/{total_count}):\n"
    result += "\n".join(formatted_items)

    if len(file_paths) > 1:
        result += f"\nFichiers concern√©s: {len(file_paths)} diff√©rents"

    return result

def format_vulnerability_data(limited_data: List[Dict], total_count: int) -> str:
    """Formate les donn√©es de vuln√©rabilit√©s"""
    if not limited_data:
        return "Aucune vuln√©rabilit√© d√©tect√©e"

    formatted_items = []
    severity_counts = {}

    for item in limited_data:
        timestamp = item.get('timestamp', 'N/A')
        agent_id = item.get('agent_id', 'N/A')

        # Extraire info vuln√©rabilit√©
        vuln_data = item.get('vulnerability', {})
        cve = vuln_data.get('cve', 'CVE_inconnu')
        severity = vuln_data.get('severity', 'Unknown')

        severity_counts[severity] = severity_counts.get(severity, 0) + 1

        summary = f"[{timestamp}] Agent:{agent_id} - {cve} ({severity})"
        formatted_items.append(summary)

    result = f"VULN√âRABILIT√âS ({len(limited_data)}/{total_count}):\n"
    result += "\n".join(formatted_items)

    # Ajouter r√©partition par s√©v√©rit√©
    if severity_counts:
        sev_summary = ", ".join([f"{sev}:{count}" for sev, count in severity_counts.items()])
        result += f"\nS√©v√©rit√©s: {sev_summary}"

    return result

def format_generic_data(limited_data: List[Dict], total_count: int) -> str:
    """Formate les donn√©es g√©n√©riques"""
    if not limited_data:
        return "Aucune donn√©e trouv√©e"

    formatted_items = []
    for item in limited_data:
        # Tentative d'extraction des champs les plus communs
        timestamp = item.get('timestamp', 'N/A')
        agent_id = item.get('agent_id', 'N/A')

        # Chercher un champ descriptif
        description = (item.get('rule_description') or
                       item.get('message') or
                       item.get('event') or
                       str(item)[:100])

        summary = f"[{timestamp}] Agent:{agent_id} - {description}"
        formatted_items.append(summary)

    result = f"DONN√âES ({len(limited_data)}/{total_count}):\n"
    result += "\n".join(formatted_items)

    return result

def prepare_mongodb_data_for_gemini(mongodb_data: Optional[List], query_type: str, question: str) -> str:
    if not mongodb_data:
        return "=== AUCUNE DONN√âE SP√âCIFIQUE TROUV√âE ==="
    
    # Gestion sp√©ciale pour les comptages simples
    if query_type in ["total_count", "agent_count"]:
        data = mongodb_data[0]
        if query_type == "total_count":
            if data.get("estimated"):
                return f"TOTAL ALERTES: ~{data['total_alerts']} (estimation)"
            else:
                return f"TOTAL ALERTES: {data['total_alerts']} (exact)"
        elif query_type == "agent_count":
            return f"AGENTS: {data['active_agents']}/{data['total_agents']} actifs, {data['disconnected_agents']} d√©connect√©s"
    
    # Utiliser le gestionnaire adaptatif pour tous les autres cas
    return data_manager.prepare_data(mongodb_data, query_type, question)


def prepare_rag_data_for_gemini(rag_results: List[Dict]) -> str:
    """Pr√©pare les donn√©es RAG pour Gemini avec limitation intelligente"""
    if not rag_results:
        return "Documentation non disponible"

    max_docs = CONFIG["gemini_max_rag_docs"]
    limited_docs = rag_results[:max_docs]

    doc_texts = []
    for doc in limited_docs:
        text = doc.get("text", "")
        truncated = truncate_text(text, 400)
        doc_texts.append(truncated)

    return " | ".join(doc_texts)

def generate_chatbot_response(question: str, formatted_data: str,
                                     system_summary: str, rag_context: str,
                                     conversation_id: str = None) -> str:
    """G√©n√©ration adaptative selon la quantit√© de donn√©es"""
    
    soc_dashboard = generate_soc_dashboard_summary()

    # Estimer la taille totale du prompt
    base_prompt_size = len(question) + len(system_summary) + len(rag_context) + len(soc_dashboard) + 1000  # +1000 pour les instructions
    data_size = len(formatted_data)
    total_size = base_prompt_size + data_size
    
    # Ajuster les instructions selon la taille
    if total_size > 6000:  # Donn√©es volumineuses
        instruction_level = "ANALYSE_CONDENSEE"
        max_tokens = 1500
    elif data_size > 2000:  # Donn√©es moyennes
        instruction_level = "ANALYSE_STANDARD"
        max_tokens = 2000
    else:  # Donn√©es l√©g√®res
        instruction_level = "ANALYSE_COMPLETE"
        max_tokens = 2500

    prompt = f"""Tu es un SOC Analyst expert Wazuh. Mode: {instruction_level}

QUESTION: "{question}"

DASHBOARD SOC: {soc_dashboard}

DONN√âES D√âTAILL√âES:
{formatted_data}

CONTEXTE SYST√àME: {system_summary}

DOCUMENTATION: {rag_context}

INSTRUCTIONS ADAPTATIVES ({instruction_level}):
{get_analysis_instructions(instruction_level)}

R√©ponse SOC analyst:"""

    return call_gemini(prompt, conversation_id, temperature=0.1, max_tokens=max_tokens)

def get_analysis_instructions(level: str) -> str:
    """Instructions adaptatives selon le volume de donn√©es"""
    
    instructions = {
        "ANALYSE_CONDENSEE": """
- Synth√®se technique pr√©cise et concise
- Actions prioritaires uniquement (top 3)
- Corr√©lations essentielles
- Format compact mais complet""",
        
        "ANALYSE_STANDARD": """
- Analyse √©quilibr√©e d√©tail/concision  
- Actions imm√©diates + recommandations
- Contexte de menace si pertinent
- Investigation si n√©cessaire""",
        
        "ANALYSE_COMPLETE": """
- Analyse approfondie de tous les √©l√©ments
- Actions d√©taill√©es avec commandes
- Recommandations pr√©ventives √©tendues
- Investigation compl√®te si incident
- Corr√©lation multi-sources avanc√©e"""
    }
    
    return instructions.get(level, instructions["ANALYSE_STANDARD"])



def process_question(question: str, custom_limits: Dict = None, 
                            conversation_id: str = None) -> str:
    """Pipeline principal optimis√© avec gestion d'erreurs robuste"""
    print(f"\n{'=' * 60}")
    print(f"Question: {question}")
    print(f"Mode: OPTIMIS√â AVEC RE-RANKER")

    try:
        # 1. Ex√©cution requ√™te MongoDB avec gestion d'erreur
        print("üîß Recherche MongoDB d√©taill√©e...")
        try:
            mongodb_results, query_type = execute_predefined_query(question)
        except Exception as e:
            print(f"‚ùå Erreur MongoDB: {e}")
            mongodb_results, query_type = [], "error"

        print("üîß Recherche documentation RAG...")
        try:
            rag_results = retrieve_with_rag(question, 
                                                   CONFIG["gemini_max_rag_docs"], 
                                                   conversation_id)
            rag_context = prepare_rag_data_for_gemini(rag_results)
        except Exception as e:
            print(f"‚ùå Erreur RAG: {e}")
            rag_context = "Documentation temporairement indisponible"

        # 3. R√©sum√© syst√®me
        print("üîß G√©n√©ration r√©sum√© syst√®me...")
        try:
            system_summary = get_mongodb_summary_compact()
        except Exception as e:
            print(f"‚ùå Erreur r√©sum√© syst√®me: {e}")
            system_summary = "R√©sum√© syst√®me temporairement indisponible"

        # 4. Formatage sp√©cialis√© des donn√©es
        print("üîß Formatage des donn√©es...")
        formatted_data = enhanced_formatter.format_data(mongodb_results or [], 
                                             query_type or "general_search", 
                                             question)

        # 5. G√©n√©ration prompt optimis√©
        optimized_prompt = generate_optimized_prompt(question, formatted_data, 
                                                   rag_context, system_summary)

        # 6. Appel Gemini avec fallback
        print("üîß G√©n√©ration r√©ponse avec IA...")
        answer = call_gemini(optimized_prompt, conversation_id, 
                                         temperature=0.1, max_tokens=2500)

        # 7. Stockage conversation
        if conversation_id:
            store_conversation(conversation_id, question, answer)

        return answer

    except Exception as e:
        print(f"‚ùå Erreur pipeline: {e}")
        import traceback
        traceback.print_exc()
        
        return f"""ERREUR SYST√àME:
Une erreur technique s'est produite lors du traitement de votre question.
Erreur: {str(e)}

ACTIONS RECOMMAND√âES:
1. V√©rifiez la connectivit√© aux services (MongoDB, Gemini API)
2. Reformulez votre question de mani√®re plus simple
3. Contactez l'administrateur syst√®me si le probl√®me persiste

Vous pouvez aussi essayer une question plus sp√©cifique comme:
- "Montrer les 10 derni√®res alertes critiques"
- "Statut des agents Wazuh"
- "√âchecs SSH des derni√®res 24h"
"""

def set_processing_limits(**kwargs):
    """Permet d'ajuster les limites de traitement"""
    for key, value in kwargs.items():
        if key in CONFIG:
            CONFIG[key] = value
            print(f"‚öôÔ∏è Limite mise √† jour: {key} = {value}")
        else:
            print(f"‚ö†Ô∏è Limite inconnue: {key}")

def get_current_limits():
    """Retourne les limites actuelles"""
    return CONFIG.copy()

def validate_system():
    """Validation du syst√®me hybride"""
    issues = []

    # Test MongoDB
    try:
        client.admin.command('ping')
        print("‚úÖ MongoDB connect√©")

        collections = db.list_collection_names()
        if not collections:
            issues.append("‚ö†Ô∏è MongoDB: Aucune collection trouv√©e")
        else:
            print(f"üìã Collections disponibles: {collections}")

            alerts_count = db["alerts"].count_documents({})
            agents_count = db["agents"].count_documents({})
            print(f"   - Alertes: {alerts_count}")
            print(f"   - Agents: {agents_count}")

    except Exception as e:
        issues.append(f"‚ùå MongoDB: {e}")

    # Test Gemini
    try:
        test_response = call_gemini("R√©ponds juste 'TEST OK'", max_tokens=10)
        if "Erreur" not in test_response:
            print("‚úÖ Gemini API op√©rationnel")
        else:
            issues.append(f"‚ùå Gemini: {test_response}")
    except Exception as e:
        issues.append(f"‚ùå Gemini: {e}")

    # Test base vectorielle
    try:
        # M√©thode plus robuste pour compter les documents
        collection_info = vectorstore.get()
        collection_count = len(collection_info.get("ids", []))
        print(f"‚úÖ Base vectorielle accessible ({collection_count} documents)")

        if collection_count < 10:
            print("‚ö†Ô∏è Base vectorielle contient peu de documents - ajout de documentation...")
            add_wazuh_documentation()

    except Exception as e:
        issues.append(f"‚ùå Vectorstore: {e}")

    if issues:
        print("\n".join(issues))
        return False

    print("üéâ Syst√®me hybride op√©rationnel")
    return True


